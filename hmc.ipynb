{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 9)\n",
    "\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy snippet to reset the global graph and global session.\n",
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter('ignore')\n",
    "  tf.reset_default_graph()\n",
    "  try:\n",
    "    sess.close()\n",
    "  except:\n",
    "    pass\n",
    "  sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is model, where we should do binary classification - predict probability of A\n",
    "\n",
    "   1) num_classes - number of classes\n",
    "   \n",
    "   2) each class has its popability of A\n",
    "   \n",
    "   3) prob - array of these probabilities; they are Q - coordinates for Hamiltonian MCMC\n",
    "   \n",
    "   4) prior (alpha, beta) are two float variables describing beta-distribution classes' popabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data():\n",
    "    ps_data_arr = np.array([\n",
    "        [20, 2, 1, 0.1],\n",
    "        [20, 2, 2, 0.1],\n",
    "        [20, 2, 3, 0.1],\n",
    "        [20, 2, 4, 0.1],\n",
    "        [20, 2, 5, 0.1],\n",
    "        [20, 2, 6, 0.1],\n",
    "        [20, 2, 7, 0.1],\n",
    "        [20, 2, 8, 0.1],\n",
    "        [20, 2, 9, 0.1],\n",
    "        [20, 2, 10, 0.1],\n",
    "        [20, 2, 11, 0.1],\n",
    "        [20, 2, 12, 0.1],\n",
    "        [20, 2, 13, 0.1],\n",
    "        [20, 2, 14, 0.1],\n",
    "        [20, 2, 15, 0.1],\n",
    "        \n",
    "        #[20, 10, 1, 0.499993], [20, 3, 2, 0.230211], [20, 8, 3, 0.236831], \n",
    "        #[20, 7, 4, 0.246463], [20, 6, 5, 0.370862], [20, 5, 6, 0.320656], \n",
    "        #[20, 10, 7, 0.519887], [20, 12, 8, 0.52845], [20, 8, 9, 0.453077], \n",
    "        #[20, 8, 10, 0.431245], [20, 10, 11, 0.499243], [20, 9, 12, 0.471968], \n",
    "        #[20, 2, 13, 0.152176], [20, 14, 14, 0.48496], [20, 6, 15, 0.246193]\n",
    "    ])\n",
    "    ps_data_pd=pd.DataFrame(data=ps_data_arr[0:, 0:],\n",
    "             index=ps_data_arr[0:, 2],\n",
    "             columns=[\"total_count\", \"clicks\", \"class_id\", \"true_p\"],\n",
    "             dtype=np.float32)\n",
    "    ps_data_pd['class_id'] = ps_data_pd.class_id.astype('int32')\n",
    "    \n",
    "    return ps_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_alpha_transform = lambda y: np.log(y)  # Not using TF here.\n",
    "fwd_alpha_transform = tf.exp\n",
    "\n",
    "def _make_ps_prior(num_classes, dtype):\n",
    "    raw_prior_alpha = tf.get_variable(\n",
    "      name='raw_prior_alpha',\n",
    "      initializer=np.array(inv_alpha_transform(5.), dtype=dtype))\n",
    "    raw_prior_beta = tf.get_variable(\n",
    "      name='raw_prior_beta',\n",
    "      initializer=np.array(inv_alpha_transform(5.), dtype=dtype))   \n",
    "    return tfd.Independent(\n",
    "      tfd.Beta(\n",
    "          fwd_alpha_transform(raw_prior_alpha) * tf.ones(num_classes),\n",
    "          fwd_alpha_transform(raw_prior_beta) * tf.ones(num_classes) * 10),\n",
    "      reinterpreted_batch_ndims=-1)#1)\n",
    "\n",
    "make_ps_prior = tf.make_template(name_='make_ps_prior', func_=_make_ps_prior)\n",
    "\n",
    "def _make_ps_log_likelihood(prob, class_id, total_count):\n",
    "    prob_c = tf.gather(prob, indices=tf.to_int32(class_id - 1), axis=-1)\n",
    "    total_count_c = tf.gather(total_count, indices=tf.to_int32(class_id - 1), axis=-1)\n",
    "    return tfp.distributions.Binomial(total_count=tf.to_float(total_count_c), probs=prob_c)\n",
    "\n",
    "make_ps_log_likelihood = tf.make_template(name_='make_ps_log_likelihood', func_=_make_ps_log_likelihood)\n",
    "\n",
    "def joint_log_prob(prob, total_count, clicks, class_id, dtype):\n",
    "    num_classes = len(total_count)\n",
    "    rv_prob = make_ps_prior(num_classes, dtype)\n",
    "    rv_clicks = make_ps_log_likelihood(prob, class_id, total_count)\n",
    "    return (rv_prob.log_prob(prob) + \n",
    "         tf.reduce_sum(rv_clicks.log_prob(clicks), axis=-1))\n",
    "\n",
    "\n",
    "def approximate_alpha_and_beta(ps_data_pd):\n",
    "    \n",
    "    dtype = np.float32\n",
    "    def unnormalized_posterior_log_prob(prob):\n",
    "        return joint_log_prob(\n",
    "            prob=tf.sigmoid(prob),\n",
    "            total_count=dtype(ps_data_pd.total_count.values),\n",
    "            clicks=dtype(ps_data_pd.clicks.values),\n",
    "            class_id=np.int32(ps_data_pd.class_id.values),\n",
    "            dtype=dtype)\n",
    "\n",
    "    step_size = tf.get_variable(\n",
    "        'step_size',\n",
    "        initializer=0.001,\n",
    "        trainable=False)\n",
    "\n",
    "    hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=15,\n",
    "        step_size=step_size,#0.01,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(target_rate=0.75),\n",
    "        state_gradients_are_stopped=True)\n",
    "\n",
    "    init_random_weights = tf.placeholder(dtype, shape=[len(ps_data_pd)])\n",
    "\n",
    "    posterior_random_weights, kernel_results = tfp.mcmc.sample_chain(\n",
    "        num_results=3,\n",
    "        num_burnin_steps=0,\n",
    "        num_steps_between_results=0,\n",
    "        current_state=init_random_weights,\n",
    "        kernel=hmc)\n",
    "\n",
    "    loss = -tf.reduce_mean(kernel_results.accepted_results.target_log_prob)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=0.1,\n",
    "        global_step=global_step,\n",
    "        decay_steps=2,\n",
    "        decay_rate=0.993)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    with tf.variable_scope('make_ps_prior', reuse=True):\n",
    "        prior_alpha = fwd_alpha_transform(tf.get_variable(\n",
    "            name='raw_prior_alpha', dtype=dtype))\n",
    "        prior_beta = fwd_alpha_transform(tf.get_variable(\n",
    "            name='raw_prior_beta', dtype=dtype))\n",
    "\n",
    "    init_op.run()\n",
    "    w_ = 0.5 * np.ones([len(ps_data_pd)], dtype=dtype)\n",
    "    \n",
    "    #print(sess.run(kernel_results, feed_dict={init_random_weights: w_}))\n",
    "    #return\n",
    "    maxiter = int(3000)\n",
    "    num_accepted = 0\n",
    "    num_drawn = 0\n",
    "    for i in range(maxiter):\n",
    "        [\n",
    "          _,\n",
    "          global_step_,\n",
    "          loss_,\n",
    "          posterior_random_weights_,\n",
    "          kernel_results_,\n",
    "          step_size_,\n",
    "          prior_alpha_,\n",
    "          prior_beta_\n",
    "        ] = sess.run([\n",
    "          train_op,\n",
    "          global_step,\n",
    "          loss,\n",
    "          posterior_random_weights,\n",
    "          kernel_results,\n",
    "          step_size,\n",
    "          prior_alpha,\n",
    "          prior_beta\n",
    "        ], feed_dict={init_random_weights: w_})\n",
    "        \n",
    "        w_ = posterior_random_weights_[-1, :]\n",
    "        num_accepted += kernel_results_.is_accepted.sum()\n",
    "        num_drawn += kernel_results_.is_accepted.size\n",
    "        acceptance_rate = num_accepted / num_drawn\n",
    "        if i % 100 == 0 or i == maxiter - 1:\n",
    "            print('global_step:{:>4}  loss:{: 7.1f}  acceptance:{:.3f}  '\n",
    "                  'step_size:{:.3f}  prior_alpha:{:.4f}  prior_beta:{:.4f}'.format(\n",
    "                      global_step_, loss_.mean(),\n",
    "                      acceptance_rate, step_size_,\n",
    "                      prior_alpha_, prior_beta_)\n",
    "            )\n",
    "            print (kernel_results_.proposed_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aapopovkin/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-bf34709ed318>:20: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-bf34709ed318>:22: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/aapopovkin/venv/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "global_step:   1  loss:  227.8  acceptance:0.556  step_size:0.001  prior_alpha:5.0000  prior_beta:5.0000\n",
      "[[0.46815774 0.455107   0.4794244  0.48353538 0.46690348 0.47342485\n",
      "  0.4832051  0.49214828 0.48649266 0.48494938 0.46424583 0.46097124\n",
      "  0.4745679  0.4910595  0.49723157]\n",
      " [0.46560255 0.49167    0.46347797 0.48222703 0.48172453 0.47486308\n",
      "  0.4525795  0.48653293 0.48042193 0.4617774  0.4826645  0.4659526\n",
      "  0.49427333 0.47372317 0.4893323 ]\n",
      " [0.45423836 0.48448035 0.49151343 0.44977427 0.46572903 0.47387683\n",
      "  0.44719264 0.4814121  0.4347256  0.435556   0.4434387  0.4776441\n",
      "  0.49481314 0.47358426 0.49979058]]\n",
      "global_step: 101  loss:  162.8  acceptance:0.697  step_size:0.000  prior_alpha:18.1902  prior_beta:1.3855\n",
      "[[0.3243078  0.28562456 0.30923986 0.27367887 0.23034884 0.39137912\n",
      "  0.09905431 0.14360753 0.23444124 0.23620631 0.2568499  0.4377275\n",
      "  0.1564266  0.42136586 0.20906126]\n",
      " [0.32213217 0.27745005 0.30107075 0.26939812 0.24155758 0.39848992\n",
      "  0.10549659 0.1371823  0.24050964 0.23952247 0.25092727 0.4365257\n",
      "  0.15217423 0.42245632 0.20530248]\n",
      " [0.31979018 0.2825254  0.30909476 0.27305013 0.2380199  0.40271112\n",
      "  0.10210988 0.1349219  0.23609814 0.240879   0.24203359 0.42895085\n",
      "  0.15083551 0.42596176 0.20655325]]\n",
      "global_step: 201  loss:  150.7  acceptance:0.726  step_size:0.000  prior_alpha:22.7290  prior_beta:1.8772\n",
      "[[0.16583525 0.10466894 0.08594657 0.17174585 0.08615545 0.36905876\n",
      "  0.07038473 0.13793528 0.22249188 0.18463601 0.12939462 0.443701\n",
      "  0.08023055 0.207899   0.20472082]\n",
      " [0.16831757 0.10331903 0.09602145 0.16846366 0.09257744 0.36280623\n",
      "  0.0690024  0.14320861 0.22609127 0.17968722 0.12114207 0.45865646\n",
      "  0.07900926 0.20419522 0.20537515]\n",
      " [0.16929184 0.10091566 0.08810724 0.16760027 0.08574403 0.36350206\n",
      "  0.0586323  0.14375249 0.22037052 0.17668028 0.12528001 0.45156494\n",
      "  0.0768626  0.22089253 0.20055674]]\n",
      "global_step: 301  loss:  135.4  acceptance:0.734  step_size:0.000  prior_alpha:26.4273  prior_beta:2.4616\n",
      "[[ 0.1306466  -0.0685995  -0.18830888  0.01633705 -0.07724157  0.27851963\n",
      "   0.02085003 -0.05639033  0.18494786 -0.06703598 -0.05191856  0.4411492\n",
      "   0.04807403  0.04006976  0.19449973]\n",
      " [ 0.13439313 -0.07747266 -0.1845309   0.02385296 -0.08381512  0.28416044\n",
      "   0.02291684 -0.06025266  0.1851718  -0.06620102 -0.0517533   0.43606257\n",
      "   0.05055932  0.03757867  0.19170342]\n",
      " [ 0.13288869 -0.07780313 -0.17436951  0.01967506 -0.08881825  0.281085\n",
      "   0.02070509 -0.06386451  0.17816757 -0.06660377 -0.04004969  0.44046578\n",
      "   0.04680079  0.0390359   0.18211484]]\n",
      "global_step: 401  loss:  122.7  acceptance:0.737  step_size:0.000  prior_alpha:28.9807  prior_beta:2.9889\n",
      "[[-0.0566928  -0.25320262 -0.13792673 -0.09922312 -0.352595    0.11123836\n",
      "   0.00567865 -0.09599641  0.00097199 -0.17709094 -0.04390338  0.29010275\n",
      "  -0.01338395  0.03459576  0.05178789]\n",
      " [-0.05771384 -0.25399515 -0.12911366 -0.09751191 -0.36507237  0.11579812\n",
      "   0.00878666 -0.08496567 -0.00752257 -0.1836739  -0.03677471  0.28373665\n",
      "  -0.00945774  0.04018837  0.04434501]\n",
      " [-0.04670014 -0.25009575 -0.13570964 -0.09744611 -0.34594345  0.1232464\n",
      "  -0.00237769 -0.08263902 -0.00068287 -0.1821761  -0.04334135  0.2782181\n",
      "  -0.00057052  0.03278313  0.04354649]]\n",
      "global_step: 501  loss:  106.9  acceptance:0.741  step_size:0.001  prior_alpha:30.9862  prior_beta:3.6543\n",
      "[[-0.23760505 -0.2273427  -0.1472436  -0.11411087 -0.3115234  -0.05807833\n",
      "  -0.30376592 -0.27529797 -0.22329462 -0.27893448 -0.30436814 -0.02719596\n",
      "  -0.18130548  0.00910098 -0.09642633]\n",
      " [-0.232959   -0.23717569 -0.13328734 -0.10898308 -0.32308212 -0.06168887\n",
      "  -0.30698556 -0.28222364 -0.22916596 -0.28581485 -0.2996805  -0.00520706\n",
      "  -0.18354672 -0.00723177 -0.10514277]\n",
      " [-0.23665915 -0.23352231 -0.12321406 -0.10673846 -0.3241149  -0.06952944\n",
      "  -0.3121891  -0.28609684 -0.23451692 -0.29646745 -0.2997922  -0.00676559\n",
      "  -0.18007296  0.00782014 -0.09434427]]\n",
      "global_step: 601  loss:   91.3  acceptance:0.743  step_size:0.001  prior_alpha:31.4142  prior_beta:4.3528\n",
      "[[-0.3801266  -0.41190502 -0.3376795  -0.23277618 -0.40170726 -0.3552134\n",
      "  -0.42435023 -0.35823834 -0.41120917 -0.5128883  -0.46972895 -0.17040345\n",
      "  -0.34154236 -0.08077152 -0.20288521]\n",
      " [-0.3828819  -0.415012   -0.34378678 -0.22416827 -0.41207927 -0.3631814\n",
      "  -0.4353431  -0.39076355 -0.40232295 -0.53279245 -0.4812855  -0.15606524\n",
      "  -0.31440195 -0.0888694  -0.21688882]\n",
      " [-0.36815903 -0.42683777 -0.34732288 -0.23293403 -0.40546843 -0.36973485\n",
      "  -0.4362226  -0.37203166 -0.39503446 -0.54774153 -0.47386524 -0.15437824\n",
      "  -0.30626404 -0.08452476 -0.2253676 ]]\n",
      "global_step: 701  loss:   77.1  acceptance:0.744  step_size:0.001  prior_alpha:30.5840  prior_beta:4.9955\n",
      "[[-0.48237202 -0.6623585  -0.4965909  -0.4962281  -0.61300564 -0.56917316\n",
      "  -0.6272609  -0.5996038  -0.481941   -0.5327838  -0.69930255 -0.21505666\n",
      "  -0.46478945 -0.1897515  -0.4290352 ]\n",
      " [-0.49251315 -0.673545   -0.4872861  -0.48844057 -0.60220456 -0.56058276\n",
      "  -0.6200673  -0.59958035 -0.49255252 -0.54182327 -0.6893539  -0.21287371\n",
      "  -0.4789801  -0.19364999 -0.4312624 ]\n",
      " [-0.49914402 -0.68374515 -0.47847357 -0.49755585 -0.61420906 -0.5513854\n",
      "  -0.6418143  -0.5959243  -0.4713332  -0.5312002  -0.69952375 -0.20680949\n",
      "  -0.47827116 -0.19599985 -0.4328895 ]]\n",
      "global_step: 801  loss:   61.9  acceptance:0.746  step_size:0.001  prior_alpha:28.5516  prior_beta:5.6415\n",
      "[[-0.746331   -0.8004122  -0.62836015 -0.60172415 -0.7193356  -0.62482893\n",
      "  -0.82325566 -0.8194236  -0.6636301  -0.9197057  -0.9017831  -0.78762114\n",
      "  -0.31790566 -0.37143436 -0.83176476]\n",
      " [-0.76454055 -0.79291576 -0.64017093 -0.62201625 -0.7494483  -0.6421131\n",
      "  -0.8232964  -0.79984295 -0.6626667  -0.9334391  -0.8916403  -0.7456713\n",
      "  -0.33527747 -0.3857438  -0.8515693 ]\n",
      " [-0.75511503 -0.8042874  -0.6286556  -0.6173649  -0.773896   -0.6437857\n",
      "  -0.82672495 -0.8227857  -0.655061   -0.93458456 -0.90506136 -0.75521845\n",
      "  -0.32841527 -0.38925362 -0.84591454]]\n",
      "global_step: 901  loss:   47.3  acceptance:0.747  step_size:0.001  prior_alpha:25.2069  prior_beta:6.2443\n",
      "[[-0.8919132  -1.1007384  -0.94432443 -1.0381635  -0.95100737 -0.6712899\n",
      "  -1.087017   -1.2196702  -0.6663101  -1.0614082  -0.996711   -1.0342394\n",
      "  -0.7126929  -0.6400387  -1.1248925 ]\n",
      " [-0.8765685  -1.1473035  -0.9523199  -1.0426455  -0.95723385 -0.6732063\n",
      "  -1.1062473  -1.2369889  -0.6857737  -1.0570319  -0.9587186  -1.0002224\n",
      "  -0.7403609  -0.62842983 -1.1621653 ]\n",
      " [-0.8799829  -1.1330571  -0.95135343 -1.0402298  -0.9563335  -0.7032291\n",
      "  -1.0770992  -1.1916652  -0.70750296 -1.0688002  -0.9492496  -1.0113547\n",
      "  -0.74325246 -0.6266505  -1.1711755 ]]\n",
      "global_step:1001  loss:   30.7  acceptance:0.748  step_size:0.001  prior_alpha:20.0560  prior_beta:7.0814\n",
      "[[-1.1319934 -1.3207346 -1.2950331 -1.2869347 -1.155109  -1.197822\n",
      "  -1.3610151 -1.2501273 -1.2132561 -1.219059  -1.2563643 -1.5377036\n",
      "  -1.3575706 -1.2711319 -1.8204323]\n",
      " [-1.120377  -1.3007886 -1.3039434 -1.2612246 -1.1923807 -1.2106739\n",
      "  -1.4040539 -1.2909834 -1.2021612 -1.2192161 -1.2391822 -1.5227298\n",
      "  -1.4022027 -1.289333  -1.868614 ]\n",
      " [-1.1151961 -1.3097112 -1.2805501 -1.2722769 -1.1944243 -1.2230839\n",
      "  -1.4144309 -1.2555708 -1.2226535 -1.2347658 -1.2399536 -1.5328075\n",
      "  -1.3730886 -1.3082556 -1.8652043]]\n",
      "global_step:1101  loss:   24.0  acceptance:0.749  step_size:0.002  prior_alpha:17.0765  prior_beta:7.7955\n",
      "[[-1.4671837 -1.3929516 -1.8177845 -1.3149183 -1.1079303 -1.54114\n",
      "  -1.7477543 -1.7566037 -1.5149901 -1.6330549 -1.7571936 -1.5753162\n",
      "  -1.5345482 -1.5120214 -1.7864082]\n",
      " [-1.5162013 -1.4142448 -1.848005  -1.3171688 -1.0912949 -1.5232713\n",
      "  -1.79664   -1.7936133 -1.4638013 -1.6343989 -1.7529593 -1.5802147\n",
      "  -1.491811  -1.5470562 -1.7735547]\n",
      " [-1.5627214 -1.39795   -1.8011047 -1.2867855 -1.0536323 -1.5371673\n",
      "  -1.7751013 -1.7985268 -1.4222687 -1.633517  -1.7184545 -1.5954182\n",
      "  -1.5174387 -1.5164696 -1.7990358]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:1201  loss:   18.1  acceptance:0.751  step_size:0.006  prior_alpha:13.7241  prior_beta:8.6209\n",
      "[[-1.9196843 -2.0785758 -1.6488721 -2.1022065 -2.183305  -1.794548\n",
      "  -1.8720657 -2.1050494 -2.4297583 -2.0107944 -1.9412618 -1.8888364\n",
      "  -1.4483483 -1.9672292 -1.9055552]\n",
      " [-1.8418076 -2.1297781 -1.6949588 -2.016657  -2.1702416 -1.9777011\n",
      "  -1.8158025 -2.068376  -2.3659928 -1.7704651 -1.9286845 -1.9978086\n",
      "  -1.693189  -2.0701723 -1.8173585]\n",
      " [-2.0321004 -2.1545675 -1.6436127 -2.113216  -2.152514  -1.8855404\n",
      "  -1.904418  -2.180688  -2.3511097 -1.9220815 -2.0416946 -1.8793217\n",
      "  -1.5495236 -2.0227284 -1.9671682]]\n",
      "global_step:1301  loss:   16.8  acceptance:0.753  step_size:0.014  prior_alpha:11.7926  prior_beta:9.3760\n",
      "[[-2.071539  -1.9316368 -2.209665  -2.1546776 -2.412686  -1.9844861\n",
      "  -2.205511  -2.2450566 -1.834626  -2.2025154 -2.030029  -2.186267\n",
      "  -2.3089383 -2.3840811 -2.4749722]\n",
      " [-2.073045  -1.9969745 -2.0918381 -2.0309675 -2.385624  -2.2989612\n",
      "  -2.264049  -2.3359315 -1.9778531 -2.424738  -2.1266484 -2.2376418\n",
      "  -2.2103384 -2.1887095 -2.3464627]\n",
      " [-1.8750355 -2.128906  -2.0980115 -2.0880601 -2.4038527 -2.2992406\n",
      "  -2.0353706 -2.3640494 -2.0497546 -2.3149889 -2.181491  -1.9992558\n",
      "  -2.3209233 -2.2444332 -2.3041975]]\n",
      "global_step:1401  loss:   17.3  acceptance:0.753  step_size:0.025  prior_alpha:11.0938  prior_beta:9.7739\n",
      "[[-2.061189  -1.8789259 -2.21686   -2.3484902 -2.246501  -1.9603673\n",
      "  -2.294288  -2.0396924 -1.9099699 -1.8604741 -2.3198693 -1.9923277\n",
      "  -2.3278153 -1.9707721 -2.2473528]\n",
      " [-2.4554532 -2.4781241 -2.2228925 -2.2063646 -2.1769397 -2.4263809\n",
      "  -2.1104558 -2.20661   -2.6250887 -1.8870642 -2.3509016 -2.098881\n",
      "  -1.8397757 -2.527766  -2.1395912]\n",
      " [-2.1750107 -2.1091332 -2.1596098 -2.2996457 -1.72748   -2.471706\n",
      "  -2.1746578 -2.0524652 -1.7415695 -1.9529396 -2.0956485 -2.1842914\n",
      "  -2.4147673 -2.019802  -2.259033 ]]\n",
      "global_step:1501  loss:   17.0  acceptance:0.755  step_size:0.059  prior_alpha:10.7928  prior_beta:9.9771\n",
      "[[-2.4436057 -2.2617812 -2.1590242 -2.1069236 -2.0795877 -1.9368049\n",
      "  -2.3352985 -2.164584  -2.5438395 -2.4676032 -2.41027   -1.9880832\n",
      "  -2.4370365 -2.098724  -2.1289344]\n",
      " [-2.2281666 -2.3716488 -2.039183  -1.9726223 -2.2660627 -1.9073422\n",
      "  -2.264823  -2.3098574 -2.2784243 -2.3663616 -2.468302  -1.765909\n",
      "  -2.5386722 -2.2904484 -1.9591588]\n",
      " [-2.2366037 -2.2653573 -2.020783  -2.0379868 -2.2205775 -2.0177274\n",
      "  -2.2014945 -2.1505945 -2.0901437 -2.5595763 -2.2884119 -1.8347554\n",
      "  -2.1216636 -2.3252308 -1.9577935]]\n",
      "global_step:1601  loss:   17.1  acceptance:0.756  step_size:0.117  prior_alpha:10.6381  prior_beta:10.1012\n",
      "[[-2.3389156 -2.5574174 -2.3224306 -2.346681  -2.4746828 -2.4295847\n",
      "  -2.2810416 -2.245338  -2.232245  -2.4018006 -2.4338279 -2.2748373\n",
      "  -2.2571151 -2.6076655 -2.1879413]\n",
      " [-2.3285074 -1.9560573 -2.3730602 -2.382993  -2.296681  -2.7998588\n",
      "  -2.2593026 -2.4927154 -2.2591333 -2.1985965 -2.223062  -2.0599842\n",
      "  -2.4472759 -2.1542199 -2.2037182]\n",
      " [-2.090077  -2.2459545 -2.4645019 -2.0237443 -2.1477873 -1.8170121\n",
      "  -2.5255873 -2.5252297 -2.3274565 -2.643474  -2.2154384 -2.346162\n",
      "  -2.0226715 -2.3827116 -2.5032876]]\n",
      "global_step:1701  loss:   18.0  acceptance:0.756  step_size:0.222  prior_alpha:10.5609  prior_beta:10.1748\n",
      "[[-2.8919342 -2.3615594 -2.603285  -2.3610103 -1.9879642 -2.4119837\n",
      "  -1.8170975 -1.837266  -2.203229  -1.8393388 -2.1907237 -2.1161041\n",
      "  -2.5711017 -2.928867  -1.7646784]\n",
      " [-1.6875958 -2.107304  -2.3829188 -2.1560876 -2.523252  -2.046925\n",
      "  -1.8478961 -1.8971483 -2.2557957 -1.9914103 -2.2821314 -2.3493285\n",
      "  -1.8860894 -2.238555  -2.6674213]\n",
      " [-2.9543016 -2.3688405 -2.1059375 -2.3422015 -2.0265434 -2.4320383\n",
      "  -2.6519382 -2.6382122 -2.2188706 -2.4890664 -2.5151892 -2.1363454\n",
      "  -2.6679146 -1.6602203 -1.9023317]]\n",
      "global_step:1801  loss:   17.7  acceptance:0.753  step_size:0.227  prior_alpha:10.4540  prior_beta:10.2388\n",
      "[[-2.4409766 -2.5389793 -2.4026213 -2.225071  -1.8717017 -2.2671034\n",
      "  -2.570841  -2.3287506 -2.6732724 -1.8176802 -2.441371  -2.3876913\n",
      "  -1.7198495 -1.8855118 -1.9154655]\n",
      " [-2.1543148 -1.9954288 -2.0694747 -2.2651043 -2.0047743 -2.2008944\n",
      "  -2.2141385 -2.182094  -1.9444507 -2.7369478 -2.0182447 -2.109935\n",
      "  -1.4614067 -2.0802593 -2.0148337]\n",
      " [-2.3326902 -2.5031183 -2.4515424 -2.230586  -2.4838147 -2.2924104\n",
      "  -1.9310988 -2.302457  -1.8993038 -1.8158293 -2.6074927 -2.3824427\n",
      "  -1.5534925 -2.211884  -2.5213912]]\n",
      "global_step:1901  loss:   18.2  acceptance:0.750  step_size:0.222  prior_alpha:10.3977  prior_beta:10.2764\n",
      "[[-2.5745645 -2.01045   -2.332809  -2.809438  -1.6722075 -1.8278096\n",
      "  -2.3669338 -2.4497993 -2.5659454 -2.126139  -2.005683  -2.2083101\n",
      "  -2.4689682 -1.7748234 -2.0073707]\n",
      " [-1.9611161 -2.0004988 -2.3086414 -1.6848826 -1.6703813 -1.9373157\n",
      "  -2.151249  -2.0193079 -1.9819437 -2.406864  -2.4683778 -2.2591531\n",
      "  -2.0004046 -1.8617067 -2.550849 ]\n",
      " [-2.5369155 -1.3966103 -2.1807811 -2.8281796 -1.7939217 -2.6687586\n",
      "  -2.3910384 -2.5870342 -2.5259292 -2.1915014 -2.065181  -2.2317905\n",
      "  -2.5139768 -2.6389172 -1.9464601]]\n",
      "global_step:2001  loss:   17.9  acceptance:0.750  step_size:0.227  prior_alpha:10.3612  prior_beta:10.3023\n",
      "[[-1.8524292 -2.158113  -2.5395055 -2.0485184 -1.915029  -2.3244252\n",
      "  -1.79016   -1.4495316 -2.2943246 -2.0503635 -2.2806704 -2.324896\n",
      "  -2.3949556 -2.00222   -2.470785 ]\n",
      " [-2.6350744 -2.3792617 -1.9607826 -2.448677  -2.567445  -2.2505426\n",
      "  -1.601503  -2.0468907 -2.2024636 -2.4292228 -2.278689  -2.1574106\n",
      "  -2.1119976 -2.470578  -2.1074693]\n",
      " [-2.0903072 -2.111167  -2.1776063 -2.102568  -1.8860489 -2.3622098\n",
      "  -2.7557437 -1.2264667 -2.2338624 -2.1920798 -2.265052  -2.333083\n",
      "  -2.5453036 -2.0714967 -2.0893643]]\n",
      "global_step:2101  loss:   16.7  acceptance:0.748  step_size:0.236  prior_alpha:10.3474  prior_beta:10.3157\n",
      "[[-2.1163566 -2.408759  -2.1488252 -2.2636728 -2.0372252 -2.1704144\n",
      "  -1.9339807 -2.3181758 -2.1454227 -2.1710703 -2.3961415 -1.9610491\n",
      "  -2.2479289 -2.3402646 -2.0232148]\n",
      " [-2.257452  -2.4560122 -2.5414078 -2.4172409 -2.4332466 -2.33793\n",
      "  -2.3991184 -2.0699673 -2.553619  -2.3966174 -2.1279082 -2.5433595\n",
      "  -2.3659081 -2.086924  -2.4330919]\n",
      " [-2.2152214 -2.1301818 -2.2325263 -2.2947218 -2.1461275 -2.2955177\n",
      "  -2.4843373 -1.9524354 -2.2209392 -2.3556826 -2.153345  -1.9851135\n",
      "  -2.16233   -2.3545709 -1.8766427]]\n",
      "global_step:2201  loss:   17.0  acceptance:0.747  step_size:0.227  prior_alpha:10.3403  prior_beta:10.3249\n",
      "[[-2.397657  -2.1823463 -2.433679  -2.3119557 -2.189694  -1.7757502\n",
      "  -2.3951485 -2.3413672 -2.4089093 -2.1125224 -2.51442   -2.3758392\n",
      "  -2.494349  -2.2390337 -2.1999998]\n",
      " [-1.8692971 -2.245801  -2.2429924 -2.2016904 -2.1659691 -2.221612\n",
      "  -1.823986  -2.3258152 -2.0114062 -2.4718382 -2.5459776 -2.2068183\n",
      "  -2.1287906 -2.439793  -2.181214 ]\n",
      " [-1.7713172 -2.4309082 -2.2688174 -2.5825639 -2.2986476 -2.197472\n",
      "  -2.693877  -2.2784507 -2.5539021 -2.3622475 -2.6075566 -2.4162152\n",
      "  -1.9477261 -2.1548598 -2.258659 ]]\n",
      "global_step:2301  loss:   17.8  acceptance:0.746  step_size:0.222  prior_alpha:10.3321  prior_beta:10.3331\n",
      "[[-2.3277948 -2.5016575 -2.5183146 -2.1466484 -1.9831228 -2.383132\n",
      "  -1.9297936 -2.3786643 -1.3143507 -2.5780442 -2.318818  -2.488346\n",
      "  -2.2776198 -1.7761763 -2.4375713]\n",
      " [-2.142578  -1.961386  -2.3838725 -2.3404293 -2.101732  -2.1318645\n",
      "  -2.0505936 -2.084701  -1.5353023 -1.9302452 -2.1568    -2.3802865\n",
      "  -2.2576525 -1.7534848 -2.1232708]\n",
      " [-2.4679167 -2.5568998 -2.132029  -2.172494  -2.6605637 -2.6047974\n",
      "  -2.5532708 -2.4201107 -1.5863047 -2.5783343 -2.3977122 -2.124513\n",
      "  -2.3004918 -1.7092335 -2.405664 ]]\n",
      "global_step:2401  loss:   17.4  acceptance:0.745  step_size:0.218  prior_alpha:10.3204  prior_beta:10.3415\n",
      "[[-2.1624367 -2.411213  -2.1160154 -2.087317  -1.9740026 -2.3315775\n",
      "  -2.2912314 -1.7841902 -2.3431628 -2.4542954 -1.9256339 -2.25614\n",
      "  -2.6464796 -2.5333633 -2.1577795]\n",
      " [-2.237756  -2.060761  -2.1833563 -2.499313  -2.3046505 -2.017838\n",
      "  -2.3375108 -2.458253  -2.0972903 -1.8526993 -2.521315  -2.5970213\n",
      "  -2.3544855 -1.7934506 -2.2154133]\n",
      " [-2.2802975 -2.0529003 -2.3460543 -1.9505725 -2.2375762 -2.6949115\n",
      "  -2.3762064 -2.644381  -2.3426194 -2.4944932 -2.3051043 -1.9048784\n",
      "  -2.26097   -1.8713055 -2.0491097]]\n",
      "global_step:2501  loss:   18.3  acceptance:0.744  step_size:0.218  prior_alpha:10.3152  prior_beta:10.3461\n",
      "[[-2.3018787 -1.8976209 -1.9598255 -2.368276  -2.2998104 -2.3147748\n",
      "  -2.1814337 -2.450107  -2.0974762 -2.3019886 -1.845353  -2.146933\n",
      "  -1.5109216 -1.7030356 -2.2055922]\n",
      " [-2.1665487 -1.8919415 -1.8717718 -2.0965967 -2.3137462 -2.1572587\n",
      "  -1.7026112 -2.0367124 -2.513982  -2.188413  -2.6179073 -2.3902528\n",
      "  -3.0051522 -1.8780017 -2.4665058]\n",
      " [-2.1965935 -2.2352662 -1.890591  -2.4135904 -2.1549442 -2.2276108\n",
      "  -1.904758  -2.379957  -1.8845166 -2.3700104 -2.6110084 -2.0291405\n",
      "  -1.6288303 -1.6425633 -2.3048508]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:2601  loss:   16.8  acceptance:0.742  step_size:0.227  prior_alpha:10.3101  prior_beta:10.3503\n",
      "[[-2.127705  -2.2006812 -2.3066254 -2.3214657 -2.4919133 -2.6348321\n",
      "  -2.250133  -2.5488405 -2.0967946 -2.4009535 -1.8958807 -2.4304047\n",
      "  -2.250201  -2.160598  -2.4170985]\n",
      " [-2.3908381 -2.1896012 -2.2938662 -2.1824362 -2.048959  -1.9884455\n",
      "  -2.2334182 -2.060388  -2.7004457 -2.167903  -1.8360181 -2.1899881\n",
      "  -2.2529943 -2.317764  -2.098547 ]\n",
      " [-2.1158347 -2.2357442 -2.1328452 -2.5042734 -2.1089292 -2.4466953\n",
      "  -2.5065885 -2.3667748 -1.8547329 -2.0283024 -1.9788839 -2.2457879\n",
      "  -2.4985852 -2.2118979 -2.00136  ]]\n",
      "global_step:2701  loss:   17.5  acceptance:0.741  step_size:0.227  prior_alpha:10.3089  prior_beta:10.3522\n",
      "[[-2.5211718 -2.118924  -2.3617535 -2.0901122 -2.6275907 -2.1114707\n",
      "  -2.2262964 -2.3381693 -2.1910598 -2.9184253 -2.220335  -2.339316\n",
      "  -2.1696796 -2.6253285 -1.6827924]\n",
      " [-1.5937129 -2.419242  -2.2427325 -2.3954582 -1.9994053 -2.355337\n",
      "  -2.24959   -2.1900282 -2.3031437 -2.0307422 -2.2571924 -2.3457131\n",
      "  -2.3607638 -1.838275  -2.84731  ]\n",
      " [-2.059645  -2.082573  -2.264961  -2.3887029 -1.9004617 -2.1656706\n",
      "  -2.2514968 -2.3326836 -2.2188506 -1.654053  -2.26893   -2.202634\n",
      "  -2.1561341 -1.9780555 -1.6731797]]\n",
      "global_step:2801  loss:   17.5  acceptance:0.740  step_size:0.227  prior_alpha:10.3062  prior_beta:10.3543\n",
      "[[-2.3677897 -2.238273  -2.3761935 -2.4203007 -2.0971448 -2.9157205\n",
      "  -2.0046816 -2.043281  -2.227551  -2.2752476 -2.3468049 -2.308042\n",
      "  -2.1507592 -2.313867  -1.6008112]\n",
      " [-2.3489103 -2.2122803 -2.0708606 -2.1274352 -2.4887116 -1.5731252\n",
      "  -2.575553  -2.3350205 -2.539435  -2.1503732 -2.3758612 -2.2021847\n",
      "  -2.3016677 -2.1034818 -1.5523403]\n",
      " [-2.3899703 -2.2471342 -2.246055  -2.016108  -2.5305767 -1.4777824\n",
      "  -1.9486477 -2.0905871 -2.1512806 -2.2164826 -2.0828786 -2.1361845\n",
      "  -2.239294  -2.3926075 -1.6421652]]\n",
      "global_step:2901  loss:   17.1  acceptance:0.739  step_size:0.227  prior_alpha:10.3048  prior_beta:10.3558\n",
      "[[-2.503407  -1.8675911 -2.068846  -2.1308997 -2.0502207 -2.752244\n",
      "  -2.174027  -2.3930728 -2.317245  -2.2016613 -2.2550879 -1.8366137\n",
      "  -2.325631  -2.2702475 -1.7459135]\n",
      " [-2.078544  -2.1481633 -2.0003436 -2.6329157 -2.8883681 -2.1443076\n",
      "  -2.2562866 -2.0646927 -2.1115422 -2.4131508 -2.0888364 -2.1585047\n",
      "  -2.3730493 -2.447477  -2.2547598]\n",
      " [-1.9414467 -1.8682423 -2.1756964 -2.3223886 -1.8059819 -1.8611032\n",
      "  -2.3169203 -2.4001453 -2.340213  -2.004863  -2.3424106 -2.0458238\n",
      "  -2.2133284 -1.9685717 -1.9389763]]\n",
      "global_step:3000  loss:   17.4  acceptance:0.739  step_size:0.224  prior_alpha:10.3042  prior_beta:10.3566\n",
      "[[-2.4247596 -2.0947347 -2.3575773 -1.9917798 -2.4632168 -2.171253\n",
      "  -1.881571  -2.4982357 -1.9141173 -2.912317  -2.1612012 -2.2101135\n",
      "  -2.7312198 -2.0058808 -2.458674 ]\n",
      " [-2.0818748 -2.3510296 -2.1189473 -2.003272  -1.8674252 -2.2729974\n",
      "  -2.1746662 -2.3397794 -2.4993422 -1.7783589 -2.3953097 -2.5021458\n",
      "  -1.796772  -2.2874365 -2.0842185]\n",
      " [-2.4573338 -2.1823115 -2.3754303 -2.4794452 -1.7725831 -2.215537\n",
      "  -2.3074446 -2.2674031 -2.005197  -1.6109594 -2.098021  -2.0325902\n",
      "  -2.7782235 -1.9250904 -2.570922 ]]\n"
     ]
    }
   ],
   "source": [
    "approximate_alpha_and_beta(get_sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "y = (x ** 15) * (1 - x) ** 100\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
