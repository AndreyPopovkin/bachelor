{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aapopovkin/venv/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 9)\n",
    "\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy snippet to reset the global graph and global session.\n",
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter('ignore')\n",
    "  tf.reset_default_graph()\n",
    "  try:\n",
    "    sess.close()\n",
    "  except:\n",
    "    pass\n",
    "  sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is model, where we should do binary classification - predict probability of A\n",
    "\n",
    "   1) num_classes - number of classes\n",
    "   \n",
    "   2) each class has its popability of A\n",
    "   \n",
    "   3) prob - array of these probabilities; they are Q - coordinates for Hamiltonian MCMC\n",
    "   \n",
    "   4) prior (alpha, beta) are two float variables describing beta-distribution classes' popabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data():\n",
    "    ps_data_arr = np.array([\n",
    "        [20, 2, 1, 0.1],\n",
    "        [20, 2, 2, 0.1],\n",
    "        [20, 2, 3, 0.1],\n",
    "        [20, 2, 4, 0.1],\n",
    "        [20, 2, 5, 0.1],\n",
    "        [20, 2, 6, 0.1],\n",
    "        [20, 2, 7, 0.1],\n",
    "        [20, 2, 8, 0.1],\n",
    "        [20, 2, 9, 0.1],\n",
    "        [20, 2, 10, 0.1],\n",
    "        [20, 2, 11, 0.1],\n",
    "        [20, 2, 12, 0.1],\n",
    "        [20, 2, 13, 0.1],\n",
    "        [20, 2, 14, 0.1],\n",
    "        [20, 2, 15, 0.1],\n",
    "        \n",
    "        #[20, 10, 1, 0.499993], [20, 3, 2, 0.230211], [20, 8, 3, 0.236831], \n",
    "        #[20, 7, 4, 0.246463], [20, 6, 5, 0.370862], [20, 5, 6, 0.320656], \n",
    "        #[20, 10, 7, 0.519887], [20, 12, 8, 0.52845], [20, 8, 9, 0.453077], \n",
    "        #[20, 8, 10, 0.431245], [20, 10, 11, 0.499243], [20, 9, 12, 0.471968], \n",
    "        #[20, 2, 13, 0.152176], [20, 14, 14, 0.48496], [20, 6, 15, 0.246193]\n",
    "    ])\n",
    "    ps_data_pd=pd.DataFrame(data=ps_data_arr[0:, 0:],\n",
    "             index=ps_data_arr[0:, 2],\n",
    "             columns=[\"total_count\", \"clicks\", \"class_id\", \"true_p\"],\n",
    "             dtype=np.float32)\n",
    "    ps_data_pd['class_id'] = ps_data_pd.class_id.astype('int32')\n",
    "    \n",
    "    return ps_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_alpha_transform = lambda y: np.log(y)  # Not using TF here.\n",
    "fwd_alpha_transform = tf.exp\n",
    "\n",
    "def _make_ps_prior(num_classes, dtype):\n",
    "    raw_prior_alpha = tf.get_variable(\n",
    "      name='raw_prior_alpha',\n",
    "      initializer=np.array(inv_alpha_transform(5.), dtype=dtype))\n",
    "    raw_prior_beta = tf.get_variable(\n",
    "      name='raw_prior_beta',\n",
    "      initializer=np.array(inv_alpha_transform(5.), dtype=dtype))   \n",
    "    return tfd.Independent(\n",
    "      tfd.Beta(\n",
    "          fwd_alpha_transform(raw_prior_alpha) * tf.ones(num_classes),\n",
    "          fwd_alpha_transform(raw_prior_beta) * tf.ones(num_classes) * 10),\n",
    "      reinterpreted_batch_ndims=-1)#1)\n",
    "\n",
    "make_ps_prior = tf.make_template(name_='make_ps_prior', func_=_make_ps_prior)\n",
    "\n",
    "def _make_ps_log_likelihood(prob, class_id, total_count):\n",
    "    prob_c = tf.gather(prob, indices=tf.to_int32(class_id - 1), axis=-1)\n",
    "    total_count_c = tf.gather(total_count, indices=tf.to_int32(class_id - 1), axis=-1)\n",
    "    return tfp.distributions.Binomial(total_count=tf.to_float(total_count_c), probs=prob_c)\n",
    "\n",
    "make_ps_log_likelihood = tf.make_template(name_='make_ps_log_likelihood', func_=_make_ps_log_likelihood)\n",
    "\n",
    "def joint_log_prob(prob, total_count, clicks, class_id, dtype):\n",
    "    num_classes = len(total_count)\n",
    "    rv_prob = make_ps_prior(num_classes, dtype)\n",
    "    rv_clicks = make_ps_log_likelihood(prob, class_id, total_count)\n",
    "    return (rv_prob.log_prob(prob) + \n",
    "         tf.reduce_sum(rv_clicks.log_prob(clicks), axis=-1))\n",
    "\n",
    "\n",
    "def approximate_alpha_and_beta(ps_data_pd):\n",
    "    \n",
    "    dtype = np.float32\n",
    "    def unnormalized_posterior_log_prob(prob):\n",
    "        return joint_log_prob(\n",
    "            prob=tf.sigmoid(prob),\n",
    "            total_count=dtype(ps_data_pd.total_count.values),\n",
    "            clicks=dtype(ps_data_pd.clicks.values),\n",
    "            class_id=np.int32(ps_data_pd.class_id.values),\n",
    "            dtype=dtype)\n",
    "\n",
    "    step_size = tf.get_variable(\n",
    "        'step_size',\n",
    "        initializer=0.001,\n",
    "        trainable=False)\n",
    "\n",
    "    hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=15,\n",
    "        step_size=step_size,#0.01,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(target_rate=0.75),\n",
    "        state_gradients_are_stopped=True)\n",
    "\n",
    "    init_random_weights = tf.placeholder(dtype, shape=[len(ps_data_pd)])\n",
    "\n",
    "    posterior_random_weights, kernel_results = tfp.mcmc.sample_chain(\n",
    "        num_results=3,\n",
    "        num_burnin_steps=0,\n",
    "        num_steps_between_results=0,\n",
    "        current_state=init_random_weights,\n",
    "        kernel=hmc)\n",
    "\n",
    "    loss = -tf.reduce_mean(kernel_results.accepted_results.target_log_prob)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=0.1,\n",
    "        global_step=global_step,\n",
    "        decay_steps=2,\n",
    "        decay_rate=0.995)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    with tf.variable_scope('make_ps_prior', reuse=True):\n",
    "        prior_alpha = fwd_alpha_transform(tf.get_variable(\n",
    "            name='raw_prior_alpha', dtype=dtype))\n",
    "        prior_beta = fwd_alpha_transform(tf.get_variable(\n",
    "            name='raw_prior_beta', dtype=dtype))\n",
    "\n",
    "    init_op.run()\n",
    "    w_ = 0.5 * np.ones([len(ps_data_pd)], dtype=dtype)\n",
    "    \n",
    "    #print(sess.run(kernel_results, feed_dict={init_random_weights: w_}))\n",
    "    #return\n",
    "    maxiter = int(3000)\n",
    "    num_accepted = 0\n",
    "    num_drawn = 0\n",
    "    for i in range(maxiter):\n",
    "        [\n",
    "          _,\n",
    "          global_step_,\n",
    "          loss_,\n",
    "          posterior_random_weights_,\n",
    "          kernel_results_,\n",
    "          step_size_,\n",
    "          prior_alpha_,\n",
    "          prior_beta_\n",
    "        ] = sess.run([\n",
    "          train_op,\n",
    "          global_step,\n",
    "          loss,\n",
    "          posterior_random_weights,\n",
    "          kernel_results,\n",
    "          step_size,\n",
    "          prior_alpha,\n",
    "          prior_beta\n",
    "        ], feed_dict={init_random_weights: w_})\n",
    "        \n",
    "        w_ = posterior_random_weights_[-1, :]\n",
    "        num_accepted += kernel_results_.is_accepted.sum()\n",
    "        num_drawn += kernel_results_.is_accepted.size\n",
    "        acceptance_rate = num_accepted / num_drawn\n",
    "        if i % 100 == 0 or i == maxiter - 1:\n",
    "            print('global_step:{:>4}  loss:{: 7.1f}  acceptance:{:.3f}  '\n",
    "                  'step_size:{:.3f}  prior_alpha:{:.4f}  prior_beta:{:.4f}'.format(\n",
    "                      global_step_, loss_.mean(),\n",
    "                      acceptance_rate, step_size_,\n",
    "                      prior_alpha_, prior_beta_)\n",
    "            )\n",
    "            print (kernel_results_.proposed_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aapopovkin/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-fdcaf1b25fa1>:20: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-fdcaf1b25fa1>:22: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/aapopovkin/venv/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "global_step:   1  loss:  227.0  acceptance:0.511  step_size:0.001  prior_alpha:5.0000  prior_beta:5.0000\n",
      "[[0.4602567  0.4866573  0.49681962 0.4723181  0.47856086 0.4798139\n",
      "  0.47910583 0.47254226 0.4808928  0.49378708 0.47938153 0.49030662\n",
      "  0.47311938 0.4813422  0.46603793]\n",
      " [0.47503036 0.49244738 0.47043794 0.5189902  0.45163402 0.4410736\n",
      "  0.43496537 0.46921536 0.43258336 0.4579056  0.4872854  0.46149376\n",
      "  0.43039894 0.45715684 0.46838525]\n",
      " [0.43313983 0.45830327 0.47396332 0.472029   0.46035695 0.49499804\n",
      "  0.45477408 0.44966516 0.454598   0.46678585 0.4449395  0.44348764\n",
      "  0.45226088 0.44281474 0.448259  ]]\n",
      "global_step: 101  loss:  155.8  acceptance:0.699  step_size:0.000  prior_alpha:17.4996  prior_beta:1.4245\n",
      "[[0.2712634  0.33424997 0.2839619  0.37763363 0.18849263 0.10546985\n",
      "  0.1243547  0.2949092  0.3470729  0.01921215 0.07021663 0.09294455\n",
      "  0.17239322 0.34280214 0.19833452]\n",
      " [0.26480275 0.33538088 0.29300287 0.3811712  0.17693926 0.10509405\n",
      "  0.12920122 0.29860628 0.33962277 0.02061977 0.06980227 0.09872232\n",
      "  0.17464413 0.34464288 0.19056694]\n",
      " [0.26975548 0.32931688 0.29527125 0.3718267  0.18375714 0.09979426\n",
      "  0.13279134 0.30136466 0.3456135  0.01408758 0.06464056 0.09179687\n",
      "  0.17424396 0.3482941  0.20780751]]\n",
      "global_step: 201  loss:  140.3  acceptance:0.727  step_size:0.000  prior_alpha:22.2240  prior_beta:2.0049\n",
      "[[ 0.19905268  0.26693907  0.14910585  0.2050407   0.02001451 -0.04809303\n",
      "  -0.0400722   0.24714388  0.26667497  0.08471298 -0.3558338  -0.04116585\n",
      "   0.01800963  0.23707789  0.1913763 ]\n",
      " [ 0.21024112  0.26442027  0.13670215  0.20189352  0.017151   -0.0527693\n",
      "  -0.03829169  0.23179342  0.27011868  0.08872588 -0.36244428 -0.04470103\n",
      "   0.01977005  0.23678207  0.20651439]\n",
      " [ 0.21488988  0.26073787  0.15342532  0.20493817  0.0091825  -0.0660418\n",
      "  -0.03255721  0.23151067  0.26971516  0.08132073 -0.3458721  -0.03925478\n",
      "   0.02378427  0.23668769  0.20866634]]\n",
      "global_step: 301  loss:  127.0  acceptance:0.733  step_size:0.000  prior_alpha:26.6769  prior_beta:2.6690\n",
      "[[ 0.10868859  0.23254068  0.0743586   0.0357952  -0.06803913 -0.19130777\n",
      "  -0.16813132  0.13023041  0.19253625  0.04444457 -0.48767084 -0.1551815\n",
      "  -0.00415755 -0.048528    0.0944273 ]\n",
      " [ 0.11312799  0.23093218  0.0692342   0.04011955 -0.06953863 -0.20783758\n",
      "  -0.17556542  0.12427532  0.18988238  0.04702799 -0.49051788 -0.1579798\n",
      "   0.00658017 -0.04440954  0.08697475]\n",
      " [ 0.10139381  0.23832282  0.06553959  0.04092759 -0.07141592 -0.2132543\n",
      "  -0.16852035  0.13087769  0.19773622  0.05007605 -0.49216667 -0.14639838\n",
      "  -0.01200036 -0.04407986  0.07468045]]\n",
      "global_step: 401  loss:  104.0  acceptance:0.738  step_size:0.001  prior_alpha:26.9542  prior_beta:3.3467\n",
      "[[ 0.01463761  0.09921424 -0.07357512 -0.12459223 -0.22312973 -0.3829114\n",
      "  -0.4661244   0.0193283   0.05226194 -0.42876992 -0.6498636  -0.34663212\n",
      "  -0.4379265  -0.3245135  -0.1872525 ]\n",
      " [ 0.00951103  0.10535373 -0.07879019 -0.09993088 -0.23034704 -0.39320698\n",
      "  -0.46362638  0.03395995  0.05761371 -0.4402903  -0.6541571  -0.33643687\n",
      "  -0.4223014  -0.3296416  -0.19304998]\n",
      " [ 0.00311616  0.10123805 -0.08660041 -0.11393435 -0.21643044 -0.37228328\n",
      "  -0.4603682   0.03727005  0.05263881 -0.44780198 -0.6626254  -0.33404335\n",
      "  -0.43192154 -0.32609192 -0.19549145]]\n",
      "global_step: 501  loss:   94.0  acceptance:0.738  step_size:0.001  prior_alpha:26.8534  prior_beta:3.7049\n",
      "[[-0.08942409  0.1651495  -0.21915385 -0.21106511 -0.34213355 -0.48274642\n",
      "  -0.49187177 -0.29611534  0.01709707 -0.46613276 -0.6981526  -0.3304708\n",
      "  -0.68239117 -0.46707946 -0.4272254 ]\n",
      " [-0.0943941   0.16693279 -0.22368993 -0.21858038 -0.33848828 -0.4888469\n",
      "  -0.49675548 -0.28910467  0.02216944 -0.4310215  -0.6965241  -0.3393974\n",
      "  -0.6513225  -0.4636921  -0.41395015]\n",
      " [-0.09583879  0.15137412 -0.2150665  -0.21826144 -0.34160462 -0.47713208\n",
      "  -0.4821666  -0.30567086  0.03905058 -0.4286013  -0.7094555  -0.33547905\n",
      "  -0.6489628  -0.46519998 -0.41625756]]\n",
      "global_step: 601  loss:   81.8  acceptance:0.738  step_size:0.001  prior_alpha:25.6143  prior_beta:4.0289\n",
      "[[-0.23659964  0.10841979 -0.21683784 -0.3507792  -0.586995   -0.6545855\n",
      "  -0.6836978  -0.3978354  -0.22132722 -0.39668652 -0.8528278  -0.5803227\n",
      "  -0.67424035 -0.6066296  -0.668356  ]\n",
      " [-0.22804697  0.1128948  -0.22079895 -0.34844193 -0.5890971  -0.674728\n",
      "  -0.67186075 -0.39392674 -0.23181188 -0.4046582  -0.84585226 -0.56697977\n",
      "  -0.7038685  -0.6226202  -0.6841714 ]\n",
      " [-0.22284174  0.11595234 -0.24726984 -0.3623132  -0.59223604 -0.6406328\n",
      "  -0.68863434 -0.38856977 -0.23739442 -0.39666077 -0.8423073  -0.56442297\n",
      "  -0.67958003 -0.6116165  -0.6839382 ]]\n",
      "global_step: 701  loss:   63.3  acceptance:0.740  step_size:0.001  prior_alpha:22.8916  prior_beta:4.4820\n",
      "[[-0.31353995 -0.33734703 -0.58103234 -0.61068344 -1.0191417  -0.9026003\n",
      "  -1.0669065  -0.58950627 -0.4463728  -0.5728739  -0.8560405  -0.7340683\n",
      "  -0.8859431  -0.61493254 -0.9854152 ]\n",
      " [-0.31531566 -0.34774628 -0.5916273  -0.6158344  -1.0189223  -0.8748951\n",
      "  -1.0517665  -0.6003259  -0.44400576 -0.5760942  -0.8453447  -0.7551976\n",
      "  -0.86179286 -0.62767655 -0.98548496]\n",
      " [-0.32631344 -0.3406621  -0.6180883  -0.62187916 -1.02763    -0.86783504\n",
      "  -1.0656265  -0.60077447 -0.44842234 -0.58394676 -0.8539108  -0.7708774\n",
      "  -0.86669475 -0.60915625 -0.9785592 ]]\n",
      "global_step: 801  loss:   50.6  acceptance:0.741  step_size:0.001  prior_alpha:20.5937  prior_beta:4.9458\n",
      "[[-0.5153099  -0.61806124 -0.8045571  -0.8869315  -1.2195959  -0.8170436\n",
      "  -1.2942613  -0.92189527 -0.74601305 -0.8737585  -0.85730326 -1.2602061\n",
      "  -1.0687507  -0.72789156 -0.85062975]\n",
      " [-0.49826407 -0.61596024 -0.8053424  -0.88143855 -1.2323925  -0.8174862\n",
      "  -1.2926425  -0.9177011  -0.73063606 -0.8796925  -0.88485724 -1.2644287\n",
      "  -1.0756872  -0.7145237  -0.84467685]\n",
      " [-0.500981   -0.60354596 -0.7955597  -0.891168   -1.2213691  -0.8384262\n",
      "  -1.320842   -0.90645564 -0.7152196  -0.8930237  -0.8730233  -1.2631166\n",
      "  -1.0911635  -0.73992497 -0.8692685 ]]\n",
      "global_step: 901  loss:   36.8  acceptance:0.742  step_size:0.001  prior_alpha:18.3734  prior_beta:5.5746\n",
      "[[-0.7707376  -1.0020801  -1.1710237  -1.1899887  -1.2439773  -1.1219083\n",
      "  -1.311703   -1.0742279  -1.1263664  -1.0383916  -1.1090246  -1.3992382\n",
      "  -1.5128937  -1.0015653  -1.3092995 ]\n",
      " [-0.7753027  -0.98664415 -1.173615   -1.193624   -1.2200557  -1.1274565\n",
      "  -1.3063334  -1.0977073  -1.1323773  -1.0440044  -1.1457216  -1.3958207\n",
      "  -1.5092891  -1.0046327  -1.3260645 ]\n",
      " [-0.8015528  -0.94891363 -1.1773406  -1.1912867  -1.2072942  -1.1239282\n",
      "  -1.3047951  -1.1076834  -1.1185929  -1.0845104  -1.1155899  -1.3655889\n",
      "  -1.552081   -1.0195799  -1.2918237 ]]\n",
      "global_step:1001  loss:   24.1  acceptance:0.744  step_size:0.002  prior_alpha:14.9074  prior_beta:6.8913\n",
      "[[-1.2212869 -1.2541667 -1.7490103 -1.4040227 -1.5509595 -1.6532547\n",
      "  -1.6990644 -1.6128758 -1.7688783 -1.9113263 -1.5556067 -1.5719138\n",
      "  -2.0107963 -1.0832746 -1.8434925]\n",
      " [-1.2683122 -1.2472796 -1.6857631 -1.4161441 -1.5097152 -1.6177305\n",
      "  -1.7148123 -1.6203929 -1.75744   -1.8738439 -1.5684357 -1.5918435\n",
      "  -1.9869255 -1.1461445 -1.8659321]\n",
      " [-1.2582532 -1.2979834 -1.7056023 -1.3795351 -1.5032978 -1.5569196\n",
      "  -1.6718445 -1.5783274 -1.7928603 -1.9015183 -1.5738367 -1.6145368\n",
      "  -2.0393672 -1.0941628 -1.8216252]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:1101  loss:   17.8  acceptance:0.748  step_size:0.008  prior_alpha:10.2021  prior_beta:8.0379\n",
      "[[-1.9212663 -2.5820298 -2.1268666 -1.6866078 -1.8946962 -1.9799445\n",
      "  -2.535628  -2.2370374 -1.9240291 -2.0540807 -1.9493351 -2.126654\n",
      "  -1.9541082 -1.7713524 -2.272952 ]\n",
      " [-2.00784   -2.6019597 -2.0439694 -1.699319  -2.0777752 -2.083114\n",
      "  -2.629895  -2.1837957 -1.9563373 -2.1448853 -2.0116343 -2.0951068\n",
      "  -1.927352  -1.6900679 -2.3292096]\n",
      " [-2.0724103 -2.6091537 -1.8955513 -1.8698119 -1.9681329 -2.0485861\n",
      "  -2.7482371 -2.2117977 -1.9180654 -2.290329  -1.8367789 -2.1072497\n",
      "  -1.9878508 -1.7890487 -2.3435483]]\n",
      "global_step:1201  loss:   17.4  acceptance:0.748  step_size:0.009  prior_alpha:8.4152  prior_beta:9.1689\n",
      "[[-2.3935905 -2.188885  -2.0824554 -2.3004515 -2.6881206 -2.5764976\n",
      "  -2.3615422 -2.8537445 -2.5504837 -2.1940434 -2.3909943 -2.2442317\n",
      "  -2.2935336 -2.592797  -2.026216 ]\n",
      " [-2.2498198 -2.1157944 -2.0810952 -2.5058005 -2.4455187 -2.472274\n",
      "  -2.4755247 -2.7310772 -2.353504  -2.0931067 -2.2401295 -2.3105145\n",
      "  -2.4020486 -2.7585185 -1.8475566]\n",
      " [-2.4017625 -2.0718818 -1.8448424 -2.5849757 -2.6706767 -2.2940032\n",
      "  -2.3572488 -2.769654  -2.453662  -1.9960474 -2.3756554 -2.348923\n",
      "  -2.2467108 -2.4683204 -2.060608 ]]\n",
      "global_step:1301  loss:   17.1  acceptance:0.749  step_size:0.015  prior_alpha:9.4976  prior_beta:9.1341\n",
      "[[-2.1427453 -2.4833431 -2.1256082 -2.3865304 -2.4522939 -2.0898783\n",
      "  -2.6638544 -2.170278  -2.3401923 -2.0913737 -2.192794  -2.6821284\n",
      "  -2.2695537 -2.78922   -2.285668 ]\n",
      " [-2.02054   -2.4453087 -2.3076189 -2.2033422 -2.4609246 -2.0269883\n",
      "  -2.4164233 -2.3519497 -2.2225163 -2.135721  -2.3080585 -2.5373244\n",
      "  -2.3056269 -2.5445786 -2.3734117]\n",
      " [-2.122725  -2.3894627 -2.0786664 -2.0671892 -2.1753395 -1.9951771\n",
      "  -2.4088926 -2.1747103 -2.244356  -2.294424  -2.2773473 -2.4859407\n",
      "  -2.245898  -2.7873957 -2.407181 ]]\n",
      "global_step:1401  loss:   17.0  acceptance:0.748  step_size:0.012  prior_alpha:9.5178  prior_beta:9.5453\n",
      "[[-2.2467277 -2.3144698 -2.42676   -2.3593428 -2.2628884 -2.2114537\n",
      "  -2.6135542 -1.9670925 -2.0689487 -2.0937164 -2.5877335 -2.4179044\n",
      "  -2.162058  -2.2059076 -2.626302 ]\n",
      " [-2.4395096 -2.0414135 -2.2963235 -2.6007903 -2.2978928 -2.1802843\n",
      "  -2.3403847 -2.0125372 -2.0996728 -2.0127215 -2.5053692 -2.107416\n",
      "  -2.3509526 -2.1044304 -2.387965 ]\n",
      " [-2.5910046 -2.0551107 -2.0985446 -2.49562   -2.4221044 -2.1407433\n",
      "  -2.344536  -2.3662019 -2.3623896 -2.196648  -2.3036516 -2.3273213\n",
      "  -2.4147182 -1.899843  -2.3087957]]\n",
      "global_step:1501  loss:   16.8  acceptance:0.748  step_size:0.014  prior_alpha:9.8371  prior_beta:9.7454\n",
      "[[-2.5030575 -2.1815557 -2.3229048 -2.5067744 -2.8098767 -2.3278942\n",
      "  -2.1286907 -2.273097  -2.1790988 -2.2391202 -2.3462794 -2.0551622\n",
      "  -2.192008  -2.2383544 -2.116132 ]\n",
      " [-2.2455447 -2.0094705 -2.2562685 -2.3955112 -2.5942006 -2.3300955\n",
      "  -2.0638623 -2.4213765 -2.357128  -2.2713935 -2.2417107 -2.2511227\n",
      "  -2.2026005 -2.0473747 -2.221561 ]\n",
      " [-2.2690766 -2.7812421 -2.3443408 -2.2923784 -2.2717104 -2.225617\n",
      "  -1.9782008 -2.4177904 -2.0480433 -2.281846  -2.3425646 -2.186399\n",
      "  -2.2961166 -2.2437172 -2.2129536]]\n",
      "global_step:1601  loss:   17.1  acceptance:0.748  step_size:0.011  prior_alpha:9.9845  prior_beta:10.0168\n",
      "[[-2.4732049 -2.5465794 -2.3826551 -2.2895794 -2.3903725 -2.3839872\n",
      "  -2.441635  -2.1871064 -2.338371  -2.2776403 -2.1782718 -2.585846\n",
      "  -2.023077  -2.294313  -2.3070955]\n",
      " [-2.265397  -2.2014341 -2.185185  -2.4124024 -2.306042  -2.3408015\n",
      "  -2.266744  -2.246814  -2.5822427 -2.0772145 -2.1695063 -2.5402658\n",
      "  -2.186826  -1.8210471 -2.3577285]\n",
      " [-2.5216312 -2.1921716 -2.0635116 -2.2321203 -2.322002  -2.210234\n",
      "  -2.500978  -2.1915855 -2.5344634 -2.0577664 -2.16335   -2.606972\n",
      "  -1.771047  -2.1233451 -2.228095 ]]\n",
      "global_step:1701  loss:   17.1  acceptance:0.748  step_size:0.013  prior_alpha:10.1816  prior_beta:10.1623\n",
      "[[-2.0848277 -2.6198509 -2.1815746 -2.442675  -2.2669556 -2.3659835\n",
      "  -2.1124454 -2.3489618 -2.711356  -2.4460685 -2.5994022 -2.3482294\n",
      "  -1.9049603 -2.4097002 -2.2198353]\n",
      " [-2.2109635 -2.7705195 -2.374191  -2.27537   -2.1239352 -2.6329737\n",
      "  -2.2930384 -2.304568  -2.6070259 -2.2336304 -2.2208574 -2.2955313\n",
      "  -1.9240289 -2.2339451 -1.98419  ]\n",
      " [-2.474991  -2.4125936 -2.41123   -2.3116026 -2.7043633 -2.387495\n",
      "  -2.597203  -2.39298   -2.1107597 -2.191572  -2.3756618 -2.309188\n",
      "  -2.081981  -2.099513  -1.9969188]]\n",
      "global_step:1801  loss:   17.0  acceptance:0.748  step_size:0.015  prior_alpha:10.4541  prior_beta:10.2311\n",
      "[[-2.0842905 -2.2214813 -2.4605255 -2.1537526 -2.6681137 -2.2431276\n",
      "  -2.2914286 -2.3719137 -2.2135932 -2.3305628 -2.1136582 -2.670199\n",
      "  -2.4364939 -2.2386582 -1.9721997]\n",
      " [-2.2955403 -2.197833  -2.1371174 -2.2650392 -2.4029562 -2.1168866\n",
      "  -2.4095094 -2.2350316 -2.2096958 -2.3996472 -2.01744   -2.2941632\n",
      "  -1.9871224 -2.1692154 -1.9464233]\n",
      " [-2.3801825 -2.5161884 -2.084262  -2.3661032 -2.4327826 -2.264591\n",
      "  -2.6158454 -2.1299741 -2.1559649 -2.1485715 -2.2042892 -2.6655116\n",
      "  -2.16906   -2.1372874 -2.2360075]]\n",
      "global_step:1901  loss:   17.0  acceptance:0.748  step_size:0.014  prior_alpha:10.3966  prior_beta:10.4210\n",
      "[[-2.318636  -2.3621674 -2.3401492 -2.5242865 -2.2981884 -2.5396605\n",
      "  -2.293479  -2.5928113 -2.1124766 -2.2011042 -2.3435113 -2.1383672\n",
      "  -2.3796568 -2.344734  -2.3526764]\n",
      " [-2.2718344 -2.221928  -2.4009676 -2.066206  -2.0256078 -2.4686203\n",
      "  -2.6975968 -2.4009783 -2.2187827 -2.1960564 -2.2680693 -2.3365963\n",
      "  -2.791643  -2.3337572 -2.1943414]\n",
      " [-2.0800047 -2.2334251 -2.1325927 -2.4857461 -2.2561107 -2.2966971\n",
      "  -2.463438  -2.165834  -1.9654046 -2.2332366 -2.4602363 -2.2472253\n",
      "  -2.669077  -2.2759159 -2.5104606]]\n",
      "global_step:2001  loss:   17.1  acceptance:0.747  step_size:0.013  prior_alpha:10.3952  prior_beta:10.5331\n",
      "[[-2.2226772 -2.3841474 -2.0830593 -2.2906096 -2.301846  -2.6309292\n",
      "  -2.3715527 -2.4548767 -2.3893166 -1.9276628 -2.434508  -2.3458698\n",
      "  -2.1085484 -2.5987709 -2.1248314]\n",
      " [-2.2613258 -2.122309  -2.2123048 -2.373974  -2.1564553 -2.5292795\n",
      "  -2.6600425 -2.4968386 -2.3345096 -2.0343063 -2.4134538 -2.7033663\n",
      "  -2.3294377 -2.4896603 -2.4986057]\n",
      " [-2.4169638 -2.339455  -2.453567  -2.4450393 -2.2169862 -2.6062815\n",
      "  -2.3721824 -2.1263497 -2.321767  -2.1716783 -2.6307993 -2.6930213\n",
      "  -2.2836752 -2.507341  -2.2210796]]\n",
      "global_step:2101  loss:   17.0  acceptance:0.747  step_size:0.014  prior_alpha:10.5027  prior_beta:10.5945\n",
      "[[-2.227364  -2.0365849 -2.2945821 -2.0984735 -2.3597953 -2.7467144\n",
      "  -2.3893912 -2.3755229 -2.2165606 -2.170465  -2.420975  -2.1166756\n",
      "  -2.2672675 -2.7937465 -2.4948974]\n",
      " [-2.0350196 -2.2848806 -2.3167806 -2.0261517 -2.3437638 -2.3209016\n",
      "  -2.59111   -2.4462674 -2.2303684 -2.2799642 -2.3633816 -2.1164317\n",
      "  -2.0418348 -2.3210785 -2.2272134]\n",
      " [-2.3814163 -2.3151352 -2.3141375 -2.307955  -2.4363172 -2.6402247\n",
      "  -2.0667698 -2.375978  -2.5543618 -2.4614027 -2.5479388 -2.2765377\n",
      "  -2.0471647 -2.1040883 -1.9443185]]\n",
      "global_step:2201  loss:   17.3  acceptance:0.747  step_size:0.019  prior_alpha:10.6170  prior_beta:10.6368\n",
      "[[-1.8857768 -2.0466638 -2.327422  -2.1409888 -2.3995228 -2.6320937\n",
      "  -2.3245087 -2.381718  -2.5457187 -2.2704883 -2.1195486 -2.5801926\n",
      "  -1.8692795 -2.1961966 -1.980714 ]\n",
      " [-2.0894163 -2.5399837 -2.0822155 -2.2997072 -2.250017  -2.2685182\n",
      "  -2.1536846 -2.3616352 -2.628929  -2.109621  -2.3249035 -2.0520835\n",
      "  -2.3857012 -2.5692408 -2.2746966]\n",
      " [-2.4771163 -2.9400344 -2.145513  -2.1489568 -2.4154084 -2.4345527\n",
      "  -2.5458484 -2.0636117 -2.123705  -2.5117502 -2.1971428 -2.3759255\n",
      "  -2.0522017 -2.0127487 -2.2999785]]\n",
      "global_step:2301  loss:   17.1  acceptance:0.746  step_size:0.012  prior_alpha:10.5986  prior_beta:10.7085\n",
      "[[-1.8642508 -2.8179812 -2.324318  -2.4654627 -2.2820115 -2.3535264\n",
      "  -2.4318533 -2.2990084 -2.4538376 -2.125659  -1.8841763 -2.219183\n",
      "  -2.3373086 -2.1199596 -2.1795635]\n",
      " [-2.1346433 -2.6069915 -2.234735  -2.3222702 -2.317163  -2.0645428\n",
      "  -2.3688478 -2.4023662 -2.1526992 -2.5046947 -2.0903528 -2.214162\n",
      "  -2.5310726 -2.152029  -2.3173127]\n",
      " [-1.9320064 -2.439975  -2.3016725 -2.1021543 -2.0317707 -1.9735334\n",
      "  -2.4127235 -2.1122649 -2.3658488 -2.702405  -2.1326058 -2.5229304\n",
      "  -2.7592206 -2.2496092 -2.2796142]]\n",
      "global_step:2401  loss:   17.4  acceptance:0.746  step_size:0.013  prior_alpha:10.6197  prior_beta:10.7499\n",
      "[[-2.2934847 -2.1101327 -2.3574088 -2.1569996 -2.3869827 -2.4583235\n",
      "  -2.2080486 -2.7083976 -2.4607832 -2.5100386 -2.1896627 -2.4985347\n",
      "  -2.5075655 -2.7513204 -2.1604164]\n",
      " [-2.2021768 -2.235731  -2.2892778 -2.5039132 -2.418685  -2.2832384\n",
      "  -2.3054273 -2.716638  -2.5379138 -2.6299577 -2.2985358 -2.4358463\n",
      "  -2.309898  -2.5489848 -2.2587285]\n",
      " [-2.3303611 -2.083399  -2.1631322 -2.442238  -2.4949305 -2.5491683\n",
      "  -2.1983721 -2.5174036 -2.4213378 -2.517747  -2.338536  -2.5261939\n",
      "  -2.504282  -2.1680615 -2.1095705]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:2501  loss:   17.5  acceptance:0.746  step_size:0.011  prior_alpha:10.6663  prior_beta:10.7579\n",
      "[[-2.4430764 -2.542497  -1.9908836 -1.8234097 -2.4628506 -2.5672333\n",
      "  -2.3941457 -2.417213  -2.4356081 -2.5502372 -2.3413355 -1.9157021\n",
      "  -2.0698915 -2.3217828 -2.2510753]\n",
      " [-2.1918046 -2.441245  -2.325936  -2.1709661 -2.7705944 -2.3136363\n",
      "  -2.4589279 -2.5521047 -2.3218353 -2.5850573 -2.6986096 -1.791299\n",
      "  -2.0719476 -2.2847111 -2.1744146]\n",
      " [-2.0818996 -2.3927376 -2.044909  -1.8736668 -2.715059  -2.3472776\n",
      "  -2.4158726 -2.426839  -2.492556  -2.6324232 -2.5946233 -2.0565803\n",
      "  -2.4628077 -2.3955355 -2.1536632]]\n",
      "global_step:2601  loss:   17.0  acceptance:0.746  step_size:0.014  prior_alpha:10.7857  prior_beta:10.7031\n",
      "[[-2.454644  -2.1364815 -2.4381874 -2.2112708 -2.3550994 -2.0567217\n",
      "  -2.2773485 -2.0890896 -2.7903774 -2.2667525 -2.2446551 -2.3841987\n",
      "  -2.5209801 -2.3269002 -2.476519 ]\n",
      " [-2.2896423 -1.9556082 -2.6398263 -2.1948497 -2.359697  -2.1122885\n",
      "  -2.2101285 -1.9556041 -2.7641532 -2.2195473 -2.174556  -2.133772\n",
      "  -2.3586845 -2.478194  -2.466512 ]\n",
      " [-2.1538    -2.1112037 -2.6557312 -2.2846472 -2.3497686 -2.3675692\n",
      "  -2.4426548 -2.250341  -2.2159982 -2.0644076 -2.2242136 -2.0515916\n",
      "  -2.2956924 -2.7153723 -2.1191807]]\n",
      "global_step:2701  loss:   17.3  acceptance:0.746  step_size:0.013  prior_alpha:10.7937  prior_beta:10.7277\n",
      "[[-2.144569  -2.3519237 -2.475107  -2.3890204 -2.1718245 -2.0466633\n",
      "  -2.2323892 -2.28045   -2.5168629 -2.1656578 -2.355679  -2.1266072\n",
      "  -2.5733929 -2.062498  -2.1158447]\n",
      " [-2.2633576 -2.5935085 -2.4346137 -2.6450772 -2.4188447 -2.1352043\n",
      "  -2.2199292 -2.2092814 -2.1442323 -2.163719  -2.2768693 -1.979564\n",
      "  -2.393068  -1.9680103 -2.4167569]\n",
      " [-1.9535085 -2.4185765 -2.3884032 -2.7646523 -2.677255  -2.2297194\n",
      "  -2.4076118 -1.9923012 -2.4326165 -2.3358998 -2.385915  -2.1196363\n",
      "  -2.5633736 -2.2572765 -2.1694343]]\n",
      "global_step:2801  loss:   16.7  acceptance:0.746  step_size:0.016  prior_alpha:10.7952  prior_beta:10.7449\n",
      "[[-2.407483  -2.316714  -2.3252277 -2.4832246 -2.2941518 -2.4557347\n",
      "  -2.3650506 -2.0611646 -2.4535077 -2.2199097 -2.1564095 -2.158805\n",
      "  -2.669535  -2.2496982 -2.1649296]\n",
      " [-2.3597293 -2.2277825 -2.4573808 -2.2662082 -2.3939877 -2.5482314\n",
      "  -2.3815098 -2.1782944 -2.2936826 -2.3360636 -1.9588957 -2.1297235\n",
      "  -2.467286  -2.3107152 -2.137144 ]\n",
      " [-2.0531592 -2.3585649 -2.3234997 -2.5111206 -2.4191918 -2.3141553\n",
      "  -2.2548642 -2.1351228 -2.4420187 -2.211003  -2.5083356 -2.1615045\n",
      "  -2.66376   -2.144024  -2.1328735]]\n",
      "global_step:2901  loss:   17.1  acceptance:0.746  step_size:0.016  prior_alpha:10.8235  prior_beta:10.7484\n",
      "[[-1.9966973 -2.2057536 -2.3327456 -2.4251995 -2.2669165 -2.4589052\n",
      "  -2.2318175 -2.319951  -2.2079072 -2.273784  -2.5765696 -2.0923297\n",
      "  -2.5232663 -2.2382684 -2.365762 ]\n",
      " [-2.2819095 -2.492899  -2.6150055 -2.1236162 -2.1621237 -2.462874\n",
      "  -2.3854184 -2.091238  -1.9913803 -2.2262456 -2.1540346 -2.3653636\n",
      "  -2.026859  -2.3786743 -1.9094249]\n",
      " [-2.3472946 -2.61487   -2.3629444 -2.5226054 -2.3852286 -1.9532771\n",
      "  -2.6193123 -2.2084365 -2.3055868 -2.324079  -2.3603392 -2.332042\n",
      "  -2.3537428 -2.3518052 -2.1130862]]\n",
      "global_step:3000  loss:   17.2  acceptance:0.746  step_size:0.018  prior_alpha:10.8386  prior_beta:10.7532\n",
      "[[-2.4717064 -2.5204365 -2.3352504 -2.2497725 -2.082773  -2.0961435\n",
      "  -2.1386106 -2.0928988 -2.028331  -2.3748627 -2.0774434 -2.2483637\n",
      "  -2.4997377 -2.4305267 -2.1700785]\n",
      " [-2.0930033 -1.8429682 -2.243539  -2.572649  -2.163481  -1.9102075\n",
      "  -2.3596418 -2.4363627 -2.2552955 -1.749653  -2.1356208 -2.3321784\n",
      "  -2.1923432 -2.2184331 -2.311    ]\n",
      " [-2.3057754 -2.335816  -2.2271023 -2.3281736 -2.424817  -2.4931755\n",
      "  -2.1302977 -2.1214337 -2.3706563 -2.1831987 -2.2155023 -2.3760746\n",
      "  -2.1814349 -2.2605782 -2.756255 ]]\n"
     ]
    }
   ],
   "source": [
    "approximate_alpha_and_beta(get_sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "y = (x ** 15) * (1 - x) ** 100\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
